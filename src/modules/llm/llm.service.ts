import { Injectable } from '@nestjs/common';
import { Observable, Subject } from 'rxjs';

@Injectable()
export class LlmService {
  /**
   * Simulates an LLM response stream with chunks.
   * @param prompt The user's input prompt
   * @returns An Observable that emits chunks of text
   */
  generateResponseStream(prompt: string): Observable<string> {
    const subject = new Subject<string>();
    
    // Calculate a random initial delay (processing time) between 1s and 3s
    // (Reduced for better UX in streaming, but overall duration will still be significant)
    const initialDelay = Math.floor(Math.random() * (3000 - 1000 + 1) + 1000);

    const fullResponse = `[Simulated AI Response] I received your message: "${prompt}". This response is being streamed to you chunk by chunk. In a real application, tokens would appear as they are generated by the LLM.`;
    
    const chunks = fullResponse.split(' ');
    let currentIndex = 0;

    setTimeout(() => {
      const interval = setInterval(() => {
        if (currentIndex >= chunks.length) {
          clearInterval(interval);
          subject.complete();
          return;
        }

        // Emit next word/chunk with a space
        const chunk = chunks[currentIndex] + (currentIndex < chunks.length - 1 ? ' ' : '');
        subject.next(chunk);
        currentIndex++;
      }, 300); // Emit a chunk every 300ms
    }, initialDelay);

    return subject.asObservable();
  }

  /**
   * Legacy one-shot response method (kept for backward compatibility if needed)
   */
  async generateResponse(prompt: string): Promise<string> {
     // Calculate a random delay between 10,000ms (10s) and 20,000ms (20s)
     const delay = Math.floor(Math.random() * (20000 - 10000 + 1) + 10000);

     // Simulate network/processing delay (non-blocking)
     await new Promise((resolve) => setTimeout(resolve, delay));
 
     // Return a hardcoded response
     return `[Simulated AI Response] I received your message: "${prompt}". This is a simulated response generated after a delay of ${delay / 1000} seconds. In a real application, this would be connected to an LLM API like OpenAI or Anthropic.`;
  }
}
